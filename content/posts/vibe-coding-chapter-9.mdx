---
title: "The Remaking of the Software Engineering Role"
date: "2026.02.16"
category: "Developer"
excerpt: "Chapter 9: The software engineer was a producer. The producer is now AI. The role is being remade."
readTime: "8 min read"
author: "sanjay"
---

## The Producer

Margaret Hamilton coined the term "software engineer" while working on Apollo. For six decades after that, the job was production. Take requirements. Write code. Ship software. The engineers who rose highest were the ones who could scale that production across teams and systems. We still hire for it, review for it, promote for it.

Then AI learned to code. An engineer at Anthropic wrote that every pull request he shipped in December — around 200 of them — was generated by AI. He called what he does now "steering." Shopify's CEO told his teams to prove a job can't be done by AI before they're allowed to hire a human for it.

AI is now the producer. The role that has existed since Hamilton coined it is being remade.

---

## The Judge

Researchers at Thoughtworks [ran experiments](https://martinfowler.com/articles/pushing-ai-autonomy.html) pushing AI agents toward autonomous code generation. One agent hit a memory error during a Docker build and increased the memory allocation instead of asking why so much memory was being used in the first place. Another agent kept adding `@JsonIgnore` annotations to suppress errors rather than fixing the underlying data model. Fowler's team described the whole experience as "whac-a-mole."

Kent Beck has been writing software for 52 years. He [started using AI agents](https://newsletter.pragmaticengineer.com/p/tdd-ai-agents-and-coding-with-kent) and found them deleting his tests to make the suite pass. He calls them "unpredictable genies" — they do what you ask, but they exploit every loophole in how you asked it.

These examples share something important. The AI produced working code each time. Builds succeeded, and the test suite was green — once the agent deleted the inconvenient tests. A junior engineer glancing at the pull request might have approved it. The problems were invisible unless you understood the system well enough to know that a memory bump was papering over a real issue, or that a deleted test was protecting an invariant.

Now think about design. When a UI is broken, it throws an error and someone files a bug. When a UI is bad — the flow is confusing, the information hierarchy is off, the interaction feels wrong — it renders fine. Nobody files a bug for "this feels off." No model will ever flag it because every pixel is exactly where the code says it should be.

The engineer who matters now is the one who looks at software that works and can tell you whether it's actually good. That's a different job than writing it.

---

## The Scaffolding

Feature pods, agile sprints, PMOs, engineering career ladders — these exist to coordinate human production at scale.

A feature pod made sense when you needed a PM to define the work, a designer to mock it up, frontend and backend engineers to build it, and a QA engineer to test it. Seven or eight people who need to communicate, resolve dependencies and stay aligned over six to eight weeks. Sprint ceremonies keep that coordination manageable. A PMO tracks dependencies across pods. Management layers keep the humans productive and unblocked.

When the producer is AI, that coordination problem mostly disappears. One engineer working with agents covers ground that used to require a pod. Cycle times drop from weeks to days, which makes two-week sprints feel like bureaucracy. Three engineers who each own an entire product surface need a Slack channel, not a program manager.

The career ladder built on scaling human production compresses with it. The path from senior engineer to manager to director to VP assumed you need many humans writing code, and therefore layers of management to coordinate them. An engineering org that goes from 90 people to 30 doesn't need six engineering managers, two directors and a VP.

Klarna cut around 700 positions and replaced them with AI. Their CEO later [admitted publicly](https://mlq.ai/news/klarna-ceo-admits-aggressive-ai-job-cuts-went-too-far-starts-hiring-again-after-us-ipo/) that the cuts "went too far" — service quality dropped because AI could not handle work that required empathy and nuanced judgment. They started hiring again. The org does change. The change is about which humans stay and what they do.

---

## The New Job Description

**Software Engineer**

You will own a product surface end to end. You will architect systems, decompose problems, orchestrate AI agents and human engineers, and decide whether what gets produced is good enough to ship. Most of your time will be spent on judgment and coordination, not writing code.

**Responsibilities:**

- Break ambiguous product problems into scoped units of work that AI agents can execute independently
- Architect systems before agents write code — define constraints, boundaries, contracts between services and agents
- Orchestrate multiple agents and human engineers working in parallel across separate contexts. Resolve conflicts between their outputs. Maintain coherence across the codebase.
- Evaluate AI-generated code for systemic correctness, not just functionality. The code will compile and the tests will pass. Your job is to catch what automation can't: broken business logic, degraded UX, local fixes that compromise the broader system.
- Maintain the processes that keep AI output reliable: plan documents, incremental verification, scope constraints

**Requirements:**

- Technical breadth across the stack. You will evaluate code in languages and frameworks you haven't used in production. Deep expertise in at least one area.
- Domain knowledge. You need to understand the business well enough to catch edge cases no test suite covers.
- Taste. You can look at a working screen and tell when something is off.
- Ability to decompose complex problems into verifiable pieces — this is how you direct agents effectively.
- Experience with AI coding tools, including their failure modes: local optimization, confident errors, self-correction traps.

**This role is not a good fit if:**

- Your only strength is deep specialization in a single framework or language, with no ability to work across the stack
- You prefer detailed tickets with clear acceptance criteria before starting work
- You measure your productivity in lines of code or pull requests merged
- You trust AI output at face value and rely on self-correction loops to fix what's wrong
- You need a PM to tell you what to build before you can start
- You don't have opinions about design or user experience
- You work on one layer of the stack and hand off the rest
- Your instinct when capacity is the problem is to hire more engineers

---

## The Canary

Software engineering is the first knowledge profession to hit this because code is what AI learned to produce first. Every knowledge profession has a production layer — contracts for lawyers, mockups for designers, models for analysts, backlogs for product managers. AI is learning to produce all of it.

When the production is handled, what is the human for? Engineering is working through that question now. Everyone else is next.
